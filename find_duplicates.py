"""
Duplicate File Finder
Findet doppelte Dateien im Download-Ordner (inkl. Unterordner) basierend auf DateigrÃ¶ÃŸe und Hash.
"""

import os
import hashlib
from pathlib import Path
from collections import defaultdict


def calculate_hash(file_path: Path, block_size: int = 65536) -> str:
    """Berechnet den SHA256-Hash einer Datei."""
    sha256 = hashlib.sha256()
    try:
        with open(file_path, 'rb') as f:
            for block in iter(lambda: f.read(block_size), b''):
                sha256.update(block)
        return sha256.hexdigest()
    except (PermissionError, OSError) as e:
        return None


def format_size(size_bytes: int) -> str:
    """Formatiert Bytes in lesbare GrÃ¶ÃŸe (KB, MB, GB)."""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if size_bytes < 1024:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024
    return f"{size_bytes:.2f} TB"


def find_duplicates(folder: Path) -> dict:
    """
    Findet doppelte Dateien in einem Ordner (rekursiv).
    
    Strategie:
    1. Gruppiere alle Dateien nach GrÃ¶ÃŸe
    2. FÃ¼r Gruppen mit >1 Datei: Berechne Hash
    3. Gruppiere nach Hash â†’ echte Duplikate
    """
    print(f"ğŸ“‚ Durchsuche: {folder}\n")
    
    # Schritt 1: Alle Dateien nach GrÃ¶ÃŸe gruppieren
    print("ğŸ” Schritt 1: Sammle Dateien und gruppiere nach GrÃ¶ÃŸe...")
    size_groups = defaultdict(list)
    file_count = 0
    
    for file_path in folder.rglob('*'):
        if file_path.is_file():
            try:
                size = file_path.stat().st_size
                size_groups[size].append(file_path)
                file_count += 1
            except (PermissionError, OSError):
                continue
    
    print(f"   âœ… {file_count} Dateien gefunden")
    
    # Nur GrÃ¶ÃŸen-Gruppen mit mehr als einer Datei behalten
    potential_duplicates = {size: files for size, files in size_groups.items() if len(files) > 1}
    potential_count = sum(len(files) for files in potential_duplicates.values())
    print(f"   ğŸ“‹ {potential_count} potenzielle Duplikate (gleiche GrÃ¶ÃŸe)\n")
    
    if not potential_duplicates:
        return {}
    
    # Schritt 2: Hash fÃ¼r potenzielle Duplikate berechnen
    print("ğŸ” Schritt 2: Berechne Hashes fÃ¼r Kandidaten...")
    hash_groups = defaultdict(list)
    processed = 0
    
    for size, files in potential_duplicates.items():
        for file_path in files:
            file_hash = calculate_hash(file_path)
            if file_hash:
                hash_groups[file_hash].append((file_path, size))
            processed += 1
            # Fortschritt anzeigen
            if processed % 50 == 0:
                print(f"   â³ {processed}/{potential_count} Dateien verarbeitet...")
    
    print(f"   âœ… {processed} Dateien gehasht\n")
    
    # Nur echte Duplikate (gleicher Hash) behalten
    duplicates = {h: files for h, files in hash_groups.items() if len(files) > 1}
    
    return duplicates


def print_report(duplicates: dict):
    """Gibt einen Bericht Ã¼ber gefundene Duplikate aus."""
    if not duplicates:
        print("=" * 60)
        print("âœ¨ Keine Duplikate gefunden!")
        print("=" * 60)
        return
    
    total_groups = len(duplicates)
    total_files = sum(len(files) for files in duplicates.values())
    total_wasted = sum((len(files) - 1) * files[0][1] for files in duplicates.values())
    
    print("=" * 60)
    print("ğŸ“Š DUPLIKATE-BERICHT")
    print("=" * 60)
    print(f"\nğŸ”¢ Gefundene Duplikat-Gruppen: {total_groups}")
    print(f"ğŸ“ Betroffene Dateien: {total_files}")
    print(f"ğŸ’¾ Verschwendeter Speicher: {format_size(total_wasted)}\n")
    print("-" * 60)
    
    for i, (file_hash, files) in enumerate(duplicates.items(), 1):
        size = files[0][1]
        print(f"\nğŸ”¹ Gruppe {i} | GrÃ¶ÃŸe: {format_size(size)} | Hash: {file_hash[:12]}...")
        print("-" * 40)
        for file_path, _ in files:
            print(f"   ğŸ“„ {file_path}")
    
    print("\n" + "=" * 60)
    print("ğŸ’¡ Tipp: ÃœberprÃ¼fe die Duplikate manuell und lÃ¶sche die nicht benÃ¶tigten.")
    print("=" * 60)


def export_report(duplicates: dict, output_file: Path):
    """Exportiert den Bericht in eine Textdatei (lesbar)."""
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("DUPLIKATE-BERICHT\n")
        f.write("=" * 60 + "\n\n")
        
        if not duplicates:
            f.write("Keine Duplikate gefunden!\n")
            return
        
        total_groups = len(duplicates)
        total_files = sum(len(files) for files in duplicates.values())
        total_wasted = sum((len(files) - 1) * files[0][1] for files in duplicates.values())
        
        f.write(f"Gefundene Duplikat-Gruppen: {total_groups}\n")
        f.write(f"Betroffene Dateien: {total_files}\n")
        f.write(f"Verschwendeter Speicher: {format_size(total_wasted)}\n\n")
        f.write("-" * 60 + "\n")
        
        for i, (file_hash, files) in enumerate(duplicates.items(), 1):
            size = files[0][1]
            f.write(f"\nGruppe {i} | GrÃ¶ÃŸe: {format_size(size)} | Hash: {file_hash[:12]}...\n")
            for file_path, _ in files:
                f.write(f"   {file_path}\n")
        
        f.write("\n" + "=" * 60 + "\n")
    
    print(f"\nğŸ“ Lesbarer Bericht: {output_file}")


def export_for_processing(duplicates: dict, output_file: Path):
    """
    Exportiert Duplikate in einem maschinenlesbaren Format fÃ¼r weitere Verarbeitung.
    
    Format:
    - Leerzeilen trennen Gruppen
    - Erste Zeile jeder Gruppe: #KEEP: (Original behalten)
    - Folgende Zeilen: #DUPLICATE: (kÃ¶nnen verschoben/gelÃ¶scht werden)
    - Jede Zeile enthÃ¤lt: Pfad|GrÃ¶ÃŸe_in_Bytes|Hash
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# DUPLIKATE-LISTE FÃœR AUTOMATISCHE VERARBEITUNG\n")
        f.write("# Format: Aktion|Pfad|GrÃ¶ÃŸe|Hash\n")
        f.write("# KEEP = Original behalten, DUPLICATE = kann verschoben werden\n")
        f.write("#" + "=" * 60 + "\n\n")
        
        for file_hash, files in duplicates.items():
            # Erste Datei als "Original" markieren (behalten)
            first_file, size = files[0]
            f.write(f"KEEP|{first_file}|{size}|{file_hash}\n")
            
            # Restliche als Duplikate markieren (kÃ¶nnen verschoben werden)
            for file_path, size in files[1:]:
                f.write(f"DUPLICATE|{file_path}|{size}|{file_hash}\n")
            
            f.write("\n")  # Leerzeile zwischen Gruppen
    
    print(f"ğŸ“‹ Verarbeitungsliste: {output_file}")


def main():
    print("=" * 60)
    print("ğŸ” Duplikat-Finder fÃ¼r Download-Ordner")
    print("=" * 60 + "\n")
    
    # Download-Ordner (Windows Standard)
    download_folder = Path.home() / "Downloads"
    
    if not download_folder.exists():
        print(f"âŒ Download-Ordner nicht gefunden: {download_folder}")
        return
    
    # Duplikate finden
    duplicates = find_duplicates(download_folder)
    
    # Bericht ausgeben
    print_report(duplicates)
    
    # Optional: Berichte exportieren
    if duplicates:
        response = input("\nğŸ“ Berichte als Textdateien speichern? (j/n): ")
        if response.lower() in ["j", "ja", "y", "yes"]:
            # Lesbarer Bericht
            report_file = download_folder / "duplikate_bericht.txt"
            export_report(duplicates, report_file)
            
            # Maschinenlesbare Liste fÃ¼r weitere Verarbeitung
            processing_file = download_folder / "duplikate_liste.txt"
            export_for_processing(duplicates, processing_file)
            
            print(f"\nğŸ’¡ Die Datei 'duplikate_liste.txt' kann von einem weiteren Skript")
            print(f"   eingelesen werden, um Duplikate automatisch zu verschieben.")


if __name__ == "__main__":
    main()
